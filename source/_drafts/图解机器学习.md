---
title: 图解机器学习
tags: machine learning
description: The key record of this book. 
mathjax: true
---
这些最新的机器学习算法，都是在最经典的算法——最小二乘法的基础上发展起来的。

### 第一部分 绪论

#### 第 1 章

使计算机获得这种泛化能力，是监督学习的最终目标。
这一类机器学习的典型任务包括：预测数值型数据的回归、预测分类标签的分类、预测顺序的排序等。

无监督学习的目标可以不是十分明确，它在人造卫星故障诊断、视频分析、社交网站解析和声音信号解析等方面大显身手。

强化学习，指在没有老师提示的情况下，自己对预测的结果进行评估的方法。

异常检测，一般情况下，在异常检测任务中，对于什么样的数据是异常的，什么样的数据是正常的，在事先是未知的。在这样的无监督的异常检测中，一般采用**密度估计**的办法，把靠近密度中心的数据作为正常的数据，偏离的作为异常的数据。

降维，从高维空间提取**关键信息**，转化为易于计算的低维问题进而求解的方法。

$\max_y p(y|x)$ 是指当 y 取得特定值的时候，$p(y|x)$ 的最大值。

$\text{argmax} \ {p(y|x)}$ 是指当 $p(y|x)$ 取得最大值时，y 对应的值。

#### 第 2 章

在线性模型中，多项式或者三角多项式等基函数与训练样本是毫不相关的。但是 **核模型** 则会在基函数设计的时候用到输入样本。

核模型，是以使用被称为 **核函数**的二元函数 $K(.,.)$，以 $K(x, x_j)_{j=1}^{n}$ 的线性结合方式加以定义的。$f_{\theta}(x) = \sum\limits_{i = 1}^{n}\theta_j K(x, x_j)$ 

在众多的核函数中，以高斯核函数的使用最为广泛：$K(x, c) = \exp{(-\dfrac{||x-c||^{2}}{2h^2})}$

与参数相关的非线性模型，称为非线性模型。其中，需要特别拿出来的，就是非线性模型中的「层级模型」。

$f_{\theta}(x) = \sum\limits_{j = 1}^{b} \alpha_j\phi(x;\beta)_j$，其中 $\alpha$ 和 $\beta$ 都是参数。

基函数通常采用 S 型函数：
$\phi(x;\beta) = \dfrac{1}{1 + \exp{(-x^{T}\omega -\gamma)}}$ 或者高斯函数 $\phi(x; \beta) = \exp{(-\dfrac{||x - c||^{2}}{2h^2})}$

S 型函数模仿的是人类脑细胞的输入输出函数，因此使用 S 型函数的层级模型也经常称为人工神经网络模型。

### 第二部分 有监督学习

#### 第 3 章 最小二乘学习法

对于 $J_{LS}(\theta) = \frac{1}{2} \sum\limits_{i = 1}^{n}(f_{\theta}(x_i) - y_i)^{2}$ 最小时的参数 $\theta$ 进行学习。其中，LS 是 Least Squares 的首字母。

随机梯度算法是指，沿着训练平方误差 $J_{LS}$ 的梯度下降，对参数 $\theta$ 依次进行学习的方法。

#### 第 4 章 带有约束条件的最小二乘法

单纯的最小二乘法对于包含噪声的学习过程经常有过拟合的弱点。这往往是由于学习模型对于训练样本而言过度复杂。下面本章将介绍能够控制模型复杂情况的、带有约束条件的最小二乘学习法。

