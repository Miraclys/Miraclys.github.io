---
title: 奇异值分解
tags: Mathematic
mathjax: true
description: Learning about SVD.
---
#### 介绍
奇异值分解是一个有着明显物理意义的方法，它可以将一个比较复杂的矩阵用更小更简单的几个子矩阵的相乘来表示，这些小矩阵描述的是矩阵的重要的特性。

就像是描述一个人一样，给别人描述说这个人长得浓眉大眼，方脸，络腮胡，而且带个黑框的眼镜，这样寥寥的几个特征，就让别人脑海里面就有一个较为清楚的认识，实际上，人脸上的特征是有着无数种的，之所以能这么描述，是因为人天生就有着非常好的抽取重要特征的能力，**让机器学会抽取重要的特征，SVD是一个重要的方法。**

在机器学习领域，有相当多的应用与奇异值都可以扯上关系，比如做feature reduction的PCA，做数据压缩（以图像压缩为代表）的算法，还有做搜索引擎语义层次检索的LSI（Latent Semantic Indexing）

#### 分析

特征值分解是一个提取矩阵特征很不错的方法，但是它只是对方阵而言的。现实生活中，我们看到的大部分矩阵都不是方阵，那么我们如何描述这样普通矩阵的重要特征呢？奇异值分解就可以来做这个事情，奇异值分解是一个能使用于任意矩阵的一种分解方式。

$\begin{aligned}
A = U \Sigma V^{T}
\end{aligned}$，其中 U 是一个 N * N 的矩阵（里面的向量是正交的，U里面的向量称为左奇异向量），$\Sigma$ 是一个 N * M 的矩阵（除了对角线的元素都是0，对角线上的元素称为奇异值），V 是一个 M * M 的矩阵（里面的向量也是正交的，V里面的向量称为右奇异向量）。

那么奇异值和特征值是如何对应起来的呢？我们对于 $(A^{T}A) \vec{v_i} = \lambda \vec{v_i}$，这里得到的 $v$ 就是上面的右奇异向量。

此外，我们还可以得到 $\sigma_i = \sqrt{\lambda_i}, u_i = \dfrac{1}{\sigma_i}A\vec{v_i}$

这里的σ就是上面说的奇异值，u就是上面说的左奇异向量。奇异值σ跟特征值类似，在矩阵Σ中也是从大到小排列，而且σ的减少特别的快，在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了。也就是说，我们也可以用前r大的奇异值来近似描述矩阵，这里定义一下部分奇异值分解：$A_{m \times n} = U_{m \times r} \Sigma_{r \times r} V^{T}_{r \times n}$，其中 r 是一个远小于 m、n 的数。

右边的三个矩阵相乘的结果将会是一个接近于A的矩阵，在这儿，r越接近于n，则相乘的结果越接近于A。而这三个矩阵的面积之和（在存储观点来说，矩阵面积越小，存储量就越小）要远远小于原始的矩阵A，我们如果想要压缩空间来表示原矩阵A，我们存下这里的三个矩阵：U、Σ、V就好了。

#### 奇异值的应用
1. 图像压缩
    我们只保留前面一定数目的比较重要的项，就可以把一个图片差不多展示出来。
2. 图像去噪
    在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪。如果一副图像包含噪声，我们有理由相信那些较小的奇异值就是由于噪声引起的。当我们强行令这些较小的奇异值为0时，就可以去除图片中的噪声。

#### 参考文章

1. https://zhuanlan.zhihu.com/p/59324612